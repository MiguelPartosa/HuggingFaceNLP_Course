{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelPartosa/HuggingFaceNLP_Course/blob/main/HuggingFaceTransformers_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtOVGqYKmQRn"
      },
      "source": [
        "# What can Transformers do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccZwLAa6mTBB"
      },
      "source": [
        "## Pipleine - the most basic Object\n",
        "- These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlNhihdTmQq6",
        "outputId": "81e2a382-9d2b-4896-aa9c-ba3725548032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvELDlatmXYw",
        "outputId": "ec5f1187-d6bf-47c6-e0e0-37e27b6ba432"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "print(classifier(\"Tummy is the best cat in the world.\"))\n",
        "print(classifier(\"tummy is the best cat in the world.\"))\n",
        "\n",
        "#They have identical scores since we defaulted to an uncased model because we didn't specify a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMqFgGzkoebn"
      },
      "source": [
        "#### Example\n",
        "Iterating overa  dataset with pipeleine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3FCdyCMoh6T"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "\n",
        "# pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
        "# dataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwmB2CnJsc8K"
      },
      "outputs": [],
      "source": [
        "# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n",
        "# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\n",
        "# output_limit = 0\n",
        "# for out in tqdm(pipe(KeyDataset(dataset, \"file\")),total=10):\n",
        "#     print(out)\n",
        "#     if output_limit >= 10:\n",
        "#         break\n",
        "#     else: output_limit +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUdD95QinBL6"
      },
      "source": [
        "## Zero-shot Classification Tasks\n",
        "- Comparing Daberta and mBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idKr9dhim-0C"
      },
      "outputs": [],
      "source": [
        "def is_medical_text(text_classifier, text, threshold=0.5):  # Default threshold of 0.5\n",
        "    result = text_classifier(text, candidate_labels=[\"medical\"])\n",
        "    score = result['scores'][0]  # Get the score for the \"medical\" label\n",
        "    return score >= threshold, score  # Return True if the score is above the threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJsRBHg7qDt9"
      },
      "source": [
        "Same base text to compare them to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stOlVEjRqDVG"
      },
      "outputs": [],
      "source": [
        "text_to_classify = \"This article discusses the side effects of a new drug.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoE0I0S9nirh"
      },
      "source": [
        "### Multilingual Bert\n",
        "Performance using Multilingual Bert for text-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91noSqMGpoO1"
      },
      "outputs": [],
      "source": [
        "mbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  # Or specify a different mBERT variant\n",
        "mbert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "mbert_text_classifier = pipeline(\"zero-shot-classification\", model=mbert_model, tokenizer=mbert_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6AH5iWnnTin"
      },
      "outputs": [],
      "source": [
        "mbert_scores = is_medical_text(mbert_text_classifier,text_to_classify)\n",
        "print(f\"Mbert Results:\\nIs medical: {mbert_scores[0]}, Score: {mbert_scores[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpnjOOzAnmMn"
      },
      "source": [
        "### Daberta\n",
        "Using Daberta to compare, we will be using the same monolingual text in english to compare the two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIc1jyIKpZKB"
      },
      "outputs": [],
      "source": [
        "daberta_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "daberta_model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\")\n",
        "\n",
        "# Create the pipeline\n",
        "daberta_text_classifier = pipeline(\"zero-shot-classification\", model=daberta_model, tokenizer=daberta_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vTrPXLFnl5x"
      },
      "outputs": [],
      "source": [
        "daberta_scores = is_medical_text(daberta_text_classifier,text_to_classify)\n",
        "print(f\"Daberta Results:\\nIs medical: {daberta_scores[0]}, Score: {daberta_scores[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1znXMbvsCFF"
      },
      "source": [
        "Comparing Both\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJY7JNj-sDl9"
      },
      "outputs": [],
      "source": [
        "#Later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3FgzKwIs5sG"
      },
      "source": [
        "## Sideline: Text-Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EtX6vEHs7oK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "\n",
        "textgen_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "textgen_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "generator = pipeline(\"text-generation\", model = textgen_model, tokenizer =textgen_tokenizer)\n",
        "\n",
        "prompt =\"in this course, we will\"\n",
        "generator(prompt, max_length=20, num_return_sequences=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9_5-XgyTKpq"
      },
      "source": [
        "# What really are transformers\n",
        "- language models trained using self-supervised learning\n",
        "- Not really good for general tasks so it is fine tuned or it goes through a process called transfer learning.\n",
        "\n",
        "[Extra source](https://medium.com/@kalra.rakshit/introduction-to-transformers-and-attention-mechanisms-c29d252ea2c5)\n",
        "> **Self-supervied Learning (SSL)** - SSL, tasks fall into two categories: pretext tasks and downstream tasks.\n",
        ">\n",
        "> **Pretext**: SSL is used to train an AI system to learn meaningful representations of unstructured data\n",
        ">\n",
        "> **Attention mechanisms** - Allows models to focus only on what's important\n",
        ">\n",
        "> **Revolution of Attention** - Allowing models to focus on important parts of input mimics the way the human mind works and enhances efficiency for the machine\n",
        ">\n",
        "> **Transformers** - operates on self attention\n",
        ">\n",
        "> **Multi-head attention** - enhances model performance by  letting the model focus on different input at one time\n",
        ">\n",
        "> **Transformers > RNNS and LSTMs** - they have parallel processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Transformer Architecture\n",
        "- The one originally designed for translation\n",
        "\n",
        "### Parts\n",
        "- **Encoder** - Consists of two layers and gets attention from the input to pass into the decoder\n",
        "- **Decoder** - Consists of three layers.\n",
        "  - **First Layer** - Attention is paid only to all past inputs\n",
        "  - **Second Layer** - Receives input from the Encoder and can pay attention to all its input\n",
        "\n",
        "\n",
        "### Architecture VS Checkpoints\n",
        "- **Architecture** - skeleton of a model\n",
        "- **Checkpoints** - weights that will be loaded on a given architecture\n",
        "- **Model** - cann be a model, can be checkpoint\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mtB-pBYiCd0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models under the Transformer Architecture\n",
        "## Encoder Models\n",
        "- Only use the encoder in a transformer architecture\n",
        "- These models are often characterized as having â€œbi-directionalâ€ attention, and are often called auto-encoding models.\n",
        "- Best for understanding ( sentence classification, NER, QnA )\n",
        "\n",
        "## Decoder Models\n",
        "- Only use the decoder in a transformer architecture\n",
        "- Often called auto-regressive models\n",
        "- Best for text Generation\n",
        "\n",
        "## Sequence-to-Sequence Models\n",
        "- Utilization of both the encoder and decoder of the transformer architecture\n",
        "- Best for text summarization, and translating"
      ],
      "metadata": {
        "id": "-T2L4pSpplKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sexism in Models ğŸ¤®"
      ],
      "metadata": {
        "id": "9m6mUq02qUBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "xlm_unmasker = pipeline(\"fill-mask\", model=\"xlm-roberta-base\")\n",
        "bert_unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "\n",
        "def mask_sexism(unmasker, text):\n",
        "    result = unmasker(text)\n",
        "    print([r[\"token_str\"] for r in result])\n",
        "\n",
        "# Let's see how roberta performs\n",
        "print(\"Mask performance with XLM:\")\n",
        "mask_sexism(xlm_unmasker,\"This man works as a <mask>.\")\n",
        "mask_sexism(xlm_unmasker,\"This woman works as a <mask>.\")\n",
        "\n",
        "# Let's see how bert base performs\n",
        "print(\"\\nMask performance with Bert Base:\")\n",
        "mask_sexism(bert_unmasker,\"This man works as a [MASK].\")\n",
        "mask_sexism(bert_unmasker,\"This woman works as a [MASK].\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn79gNWOqXL3",
        "outputId": "e7b59b6e-9c81-4764-cbc6-84448cf8dc3f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask performance with XLM:\n",
            "['guide', 'doctor', 'model', 'robot', 'teacher']\n",
            "['model', 'teacher', 'lawyer', 'escort', 'doctor']\n",
            "\n",
            "Mask performance with Bert Base:\n",
            "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
            "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2Spgw6Nt0nH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SMqFgGzkoebn"
      ],
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeVdR5eMONGhRCwm61HikW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}